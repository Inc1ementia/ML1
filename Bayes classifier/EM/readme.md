[К меню](https://github.com/Inc1ementia/ML1)

# Метод Expectation-Maximization

*Байесовский подход* является классическим в теории распознавания образов и лежит в основе многих методов. Он опирается на теорему о том, что если плотности распределения классов известны, то алгоритм классификации, имеющий минимальную вероятность ошибок, можно выписать в явном виде.

На практике плотности распределения классов, как правило, не известны. Их приходится оценивать (восстанавливать) по обучающей выборке. В результате байесовский алгоритм перестаёт быть оптимальным, так как восстановить плотность по выборке можно только с некоторой погрешностью. Чем короче выборка, тем выше шансы подогнать распределение под конкретные данные и столкнуться с эффектом переобучения.

ЕМ-алгоритмом (expectation-maximization) принято называть довольно работоспособную схему построения процедур итерационного типа для численного решения задачи поиска экстремума целевой функции в разнообразных задачах оптимизации. В частности, в прикладной статистике эта схема довольно эффективна для поиска оценок максимального правдоподобия и родственных им в ситуациях, когда функция правдоподобия имеет сложную структуру, из-за которой другие методы оказываются неэффективными или вообще не применимыми.

EM-алгоритм состоит из итерационного повторения двух шагов. На E-шаге вычисляется ожидаемое значение (expectation) вектора скрытых переменных G по текущему приближению вектора параметров Θ. На М-шаге решается задача максимизации правдоподобия (maximization) и находится следующее приближение вектора Θ по текущим значениям векторов G и Θ.

E-шаг (expectation):

<img src="https://render.githubusercontent.com/render/math?math=%7Bg%7D_%7Bij%7D%5E%7B0%7D%3A%3D%7Bg%7D_%7Bij%7D%3B%5Cquad%20%7Bg%7D_%7Bij%7D%3A%3D%5Cfrac%7B%7Bw%7D_%7Bj%7D%5Cvarphi(%20%7Bx%7D_%7Bi%7D%3B%7B%5Ctheta%7D_%7Bj%7D)%7D%7B%5Csum_%7Bs%3D1%7D%5E%7Bk%7D%20%7Bw%7D_%7Bs%7D%5Cvarphi(%7Bx%7D_%7Bi%7D%3B%7B%5Ctheta%7D_%7Bs%7D)%7D%20%5Cqquad%20%5Cforall%20i%20%3D%201%2C%20%5Cdots%2C%20m%3B%20%5Cqquad%20%5Cforall%20j%20%3D%201%2C%20%5Cdots%2C%20k">

M-шаг (maximization): 

<img src="https://render.githubusercontent.com/render/math?math=%7B%5Ctheta%7D_%7Bj%7D%3A%3D%5Carg%5Cmax_%7B%5Ctheta%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20%7Bg%7D_%7Bij%7D%5Cln%5Cvarphi(%20%7Bx%7D_%7Bi%7D%3B%7B%5Ctheta%7D)%3B%5Cquad%20%7Bw%7D_%7Bj%7D%3A%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20%7Bg%7D_%7Bij%7D%20%5Cqquad%20%5Cforall%20j%20%3D%201%2C%5Cdots%2C%20k">


### Программная реализация алгоритма

```R
muHat <- function(xl) {   #считаем значение мю по данным для класса
  n <- dim(xl)[2]
  mu <- array(NA,n)
  for (i in 1:n) {
    mu[i] <- mean(xl[ ,i])   #по каждой координате берём среднее
  }
  return (t(mu))
}


sigmaHat <- function(xl,mu) {   #считаем зачение сигма по данным для класса и полученному мю
  l <- dim(xl)[1]
  n <- dim(xl)[2]
  sigma <- matrix(0,n,n)
  for (i in 1:l) {
    sigma <- sigma+(t(xl[i, ]-mu) %*% (xl[i, ]-mu))/(l-1)
  }
  return (sigma)
}


naive <- function(mu,sigma,obj,lambda,P) {   #наивный Байес
  n <- length(mu)
  res <- log(lambda*P)   #добавочный коэффициент
  params <- array(NA,n)
  for (i in 1:n) {
    params[i] <- sigma[i,i]   #диагональные элементы матрицы (гипотеза наивного классификатора)
  }
  return (res+sum(log(Pyj(params,obj,c(mu)))))   #значение функции правдоподобия
}


Pyj <- function(sj,xj,muj){    #коэффициент для j-ой компоненты
  return ((1.0/(sj*sqrt(2*pi)))*exp(-((xj-muj)^2)/(2*sj^2)))    #формула нормального наивного Байесовского классификатора
}
```

### Результат работы алгоритма

Результатом работы алгоритма будет следующий график:

![EM](EM.png)

[К меню](https://github.com/Inc1ementia/ML1)
