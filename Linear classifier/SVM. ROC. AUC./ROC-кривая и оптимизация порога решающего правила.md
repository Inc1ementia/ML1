# ROC-кривая и оптимизация порога решающего правила

Рассмотрим задачу классификации на два класса, Y = {−1, +1}, и модель алгоритмов
$$
a(x, w) = sign(f(x, w) − w_0)
$$
где 
$$
w_0\in\mathbb{R}
$$
аддитивный параметр дискриминантной функции. В теории нейронных сетей его называют *порогом активации*.

В случае линейной дискриминантной функции параметр определяется отношением потерь:
$$
{w}_{0}=\ln\frac{{\lambda}_{-}}{{\lambda}_{+}}
$$
где λ+ и λ− — величина потери при ошибке на объекте класса «+1» и «−1» соответственно.

На практике отношение потерь может многократно пересматриваться. Поэтому вводится специальная характеристика — *ROC-кривая*, которая показывает, что происходит с числом ошибок обоих типов, если изменяется отношение потерь. Термин *операционная характеристика приёмника* (receiver operating characteristic, ROC curve) пришёл из теории обработки сигналов.

Каждая точка на ROC-кривой соответствует некоторому алгоритму. В общем случае это даже не обязательно кривая — дискретное множество алгоритмов может быть отображено в тех же координатах в виде точечного графика.

По оси X откладывается доля *ошибочных положительных классификаций* (false positive rate, FPR)
$$
\operatorname{FPR}t( a,{X}^{l}) =\frac{{\sum}_{i=1}^{l}[{y}_{i}=-1][a({x}_{i}) =+1]}{{\sum}_{i=1}^{l}[ {y}_{i}=-1]}
$$
Величина 1 − FPR(a) равна доле *правильных отрицательных классификаций* (true negative rate, TNR) и называется *специфичностью* алгоритма. Поэтому на горизонтальной оси иногда пишут «1 − специфичность».

По оси Y откладывается доля *правильных положительных классификаций* (true positive rate, TPR), называемая также *чувствительностью* алгоритма:
$$
\operatorname{TPR}( a,{X}^{l}) =\frac{{\sum}_{i=1}^{l}[{y}_{i}=+1][a({x}_{i}) =+1]}{{\sum}_{i=1}^{l}[{y}_{i}=+1]}
$$
Каждая точка ROC-кривой соответствует определённому значению параметра. ROC-кривая монотонно не убывает и проходит из точки (0, 0) в (1, 1).

Для построения ROC-кривой нет необходимости вычислять FPR и TPR суммированием по всей выборке при каждом параметре. Более эффективный алгоритм основан на простой идее, что в качестве значений порога достаточно перебрать только значения дискриминантной функции
$$
f({x}_{i})=\langle w, {x}_{i} \rangle
$$
которые она принимает на объектах выборки.

Чем выше проходит ROC-кривая, тем выше качество классификации. Идеальная ROC-кривая проходит через левый верхний угол — точку (0, 1). Наихудший алгоритм соответствует диагональной прямой, соединяющей точки (0, 0) и (1, 1); её также изображают на графике как ориентир.

В роли общей характеристики качества классификации, не зависящей от конъюнктурного параметра, выступает *площадь под ROC-кривой* (area under curve, AUC). 

### Алгоритм

------

**Вход:** 

​	обучающая выборка:
$$
{X}^{l}
$$
​	дискриминантная функция:

$$
f(x)=\langle w, x \rangle
$$
**Выход:** 

​	последовательность точек ROC-кривой:
$$
{\{({\operatorname{FPR}}_{i},{\operatorname{TPR}}_{i})\}}_{i=0}^{l}
$$
​	площадь под ROC-кривой: AUC

------

 1. число объектов класса −1:
    $$
    {l}_{-}:={\sum}_{i=1}^{l}[{y}_{i}=-1]
    $$
    число объектов класса +1:
    $$
    {l}_{+}:={\sum}_{i=1}^{l}[{y}_{i}=+1]
    $$

 2. упорядочить выборку по убыванию значений
    $$
    f({x}_{i})
    $$

 3. поставить первую точку в начало координат:
    $$
    ({\operatorname{FPR}}_{0},{\operatorname{TPR}}_{0}):=(0,0);\quad \operatorname{AUC}:=0
    $$

 4. $$
    i:=1,\dots,l
    $$

    если 
    $$
    {y}_{i}=−1
    $$
    то сместиться на один шаг вправо:
    $$
    {\operatorname{FPR}}_{i} := {\operatorname{FPR}}_{i-1} +\frac{1}{{l}_{-}}; \quad {\operatorname{TPR}}_{i} := {\operatorname{TPR}}_{i-1}
    $$

    $$
    \operatorname{AUC} := \operatorname{AUC}+\frac{1}{{l}_{-}} {\operatorname{TPR}}_{i}
    $$

    иначе сместиться на один шаг вверх:
    $$
    {\operatorname{FPR}}_{i} := {\operatorname{FPR}}_{i-1};\;\;{\operatorname{TPR}}_{i} :={\operatorname{TPR}}_{i-1} +\frac{1}{{l}_{+}}
    $$