# Метод опорных векторов

**Метод опорных векторов** (англ. support vector machine, SVM) — один из наиболее популярных методов обучения, который применяется для решения задач классификации и регрессии. Основная идея метода заключается в построении *оптимальной разделяющей гиперплоскости*, делящей объекты выборки оптимальным способом.

Метод SVM обладает несколькими замечательными свойствами. Во-первых, обучение SVM сводится к задаче квадратичного программирования, имеющей единственное решение, которое вычисляется достаточно эффективно даже на выборках в сотни тысяч объектов. Во-вторых, решение обладает свойством разреженности: положение оптимальной разделяющей гиперплоскости зависит лишь от небольшой доли обучающих объектов. Они и называются опорными векторами; остальные объекты фактически не задействуются. Наконец, с помощью изящного математического приёма — введения функции ядра — метод обобщается на случай нелинейных разделяющих поверхностей. Вопрос о выборе ядра, оптимального для данной прикладной задачи, до сих пор остаётся открытой теоретической проблемой.

### Линейно разделимая выборка

Предположим, что выборка

<img src="https://render.githubusercontent.com/render/math?math=%7BX%7D%5E%7Bl%7D%3D%7B(%20%7Bx%7D_%7Bi%7D%2C%7By%7D_%7Bi%7D)%7D_%7Bi%3D1%7D%5E%7Bl%7D">

линейно разделима и существуют значения параметров, при которых функционал числа ошибок

<img src="https://render.githubusercontent.com/render/math?math=Q(%20w%2C%7Bw%7D_%7B0%7D)%20%3D%5Csum_%7Bi%3D1%7D%5E%7Bl%7D%5B%20%7By%7D_%7Bi%7D(%20%5Clangle%20w%2C%20%7Bx%7D_%7Bi%7D%5Crangle%20-%7Bw%7D_%7B0%7D)%20%5Cleq%200%5D">

принимает нулевое значение. Но тогда разделяющая гиперплоскость не единственна. Можно выбрать другие её положения, реализующие такое же разбиение выборки на два класса. Идея метода заключается в том, чтобы разумным образом распорядиться этой свободой выбора.

###### Оптимальная разделяющая гиперплоскость:

Потребуем, чтобы разделяющая гиперплоскость максимально далеко отстояла от ближайших к ней точек обоих классов. Первоначально данный принцип классификации возник из эвристических соображений: вполне естественно полагать, что максимизация *зазора* (margin) между классами должна способствовать более надёжной классификации.

###### Нормировка:

Заметим, что параметры линейного порогового классификатора определены с точностью до нормировки: алгоритм не изменится, если параметры одновременно умножить на одну и ту же положительную константу. Удобно выбрать эту константу таким образом, чтобы выполнялось условие

<img src="https://render.githubusercontent.com/render/math?math=%5Cmin%5Climits_%7Bi%3D1%2C%5Cdots%2Cl%7D%20%7By%7D_%7Bi%7D(%20%5Clangle%20w%2C%7Bx%7D_%7Bi%7D%5Crangle%20-%20%7Bw%7D_%7B0%7D)%20%3D1">

Множество точек

<img src="https://render.githubusercontent.com/render/math?math=%5C%7Bx%5Ccolon%20-1%5Cleq%20%5Clangle%20w%2Cx%5Crangle%20-%7Bw%7D_%7B0%7D%5Cleq%201%5C%7D">

описывает полосу, разделяющую классы. Ни один из объектов обучающей выборки не попадает внутрь этой полосы. Границами полосы служат две параллельные гиперплоскости с вектором нормали w. Разделяющая гиперплоскость проходит ровно по середине между ними. Объекты, ближайшие к разделяющей гиперплоскости, лежат на границах полосы, и именно на них достигается минимум. В каждом из классов имеется хотя бы один такой объект, в противном случае разделяющую полосу можно было бы ещё немного расширить и нарушался бы принцип максимального зазор.

###### Ширина разделяющей полосы:

Чтобы разделяющая гиперплоскость как можно дальше отстояла от точек выборки, ширина полосы должна быть максимальной. Пусть x− и x+ — два обучающих объекта классов −1 и +1 соответственно, лежащие на границе полосы. Тогда ширина полосы есть

<img src="https://render.githubusercontent.com/render/math?math=%5Clangle(%20%7Bx%7D_%7B%2B%7D%20-%7Bx%7D_%7B-%7D)%20%2C%5Cfrac%7Bw%7D%7B%5ClVert%20w%5CrVert%7D%20%5Crangle%20%3D%5Cfrac%7B%5Clangle%20w%2C%7Bx%7D_%7B%2B%7D%5Crangle%20-%5Clangle%20w%2C%7Bx%7D_%7B-%7D%5Crangle%7D%7B%5ClVert%20w%5CrVert%7D%20%3D%5Cfrac%7B(%7Bw%7D_%7B0%7D%20%2B1)%20-(%7Bw%7D_%7B0%7D%20-1)%7D%7B%5ClVert%20w%5CrVert%7D%20%3D%5Cfrac%7B2%7D%7B%5ClVert%20w%5CrVert%7D">

Ширина полосы максимальна, когда норма вектора w минимальна. Итак, в случае линейно разделимой выборки получаем задачу квадратичного программирования: требуется найти значения параметров, при которых выполняются l ограничений-неравенств и норма вектора w минимальна:

<img src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7Bcases%7D%0A%5Clangle%20w%2Cw%5Crangle%20%5Crightarrow%20%5Cmin%20%3B%5C%5C%0A%7By%7D_%7Bi%7D(%5Clangle%20w%2C%7Bx%7D_%7Bi%7D%5Crangle%20-%7Bw%7D_%7B0%7D)%5Cgeq%201%2C%5Cquad%20i%3D1%2C%5Cdots%2Cl.%0A%5Cend%7Bcases%7D">

На практике линейно разделимые классы встречаются довольно редко. Поэтому постановку задачи необходимо модифицировать так, чтобы система ограничений была совместна в любой ситуации.

### Линейно неразделимая выборка

Чтобы обобщить постановку задачи на случай линейно неразделимой выборки, позволим алгоритму допускать ошибки на обучающих объектах, но при этом постараемся, чтобы ошибок было поменьше. Введём дополнительные переменные, характеризующие величину ошибки на объектах. Ослабим ограничения-неравенства и одновременно введём в минимизируемый функционал штраф за суммарную ошибку:

<img src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7Bcases%7D%0A%5Cfrac%7B1%7D%7B2%7D%5Clangle%20w%2Cw%5Crangle%20%2BC%5Csum_%7Bi%3D1%7D%5E%7Bl%7D%20%7B%5Cxi%7D_%7Bi%7D%5Crightarrow%20%5Cmin%5Climits_%7Bw%2C%7Bw%7D_%7B0%7D%2C%5Cxi%7D%3B%5C%5C%0A%7By%7D_%7Bi%7D(%5Clangle%20w%2C%7Bx%7D_%7Bi%7D%5Crangle%20-%7Bw%7D_%7B0%7D)%5Cgeq%201-%7B%5Cxi%7D_%7Bi%7D%2C%5Cquad%20i%3D1%2C%5Cdots%2Cl%3B%5C%5C%0A%7B%5Cxi%7D_%7Bi%7D%5Cgeq%200%2C%5Cquad%20i%3D1%2C%5Cdots%2Cl.%0A%5Cend%7Bcases%7D">

Положительная константа C является управляющим параметром метода и позволяет находить компромисс между максимизацией ширины разделяющей полосы и минимизацией суммарной ошибки.

###### Регуляризация эмпирического риска:

В задачах с двумя классами Y = {−1, +1} *отступом* (margin) объекта от границы классов называется величина

<img src="https://render.githubusercontent.com/render/math?math=M_i(w%2C%7Bw%7D_%7B0%7D)%20%3D%7By%7D_%7Bi%7D((%20w%2C%20%7Bx%7D_%7Bi%7D)%20-%7Bw%7D_%7B0%7D)">

Алгоритм допускает ошибку на объекте тогда и только тогда, когда отступ отрицателен. Если отступ ∈ (−1, +1), то объект попадает внутрь разделяющей полосы. Если отступ > 1, то объект классифицируется правильно, и находится на некотором удалении от разделяющей полосы.

Таким образом, задача линейно неразделимой выборки оказывается эквивалентной безусловной минимизации функционала:

<img src="https://render.githubusercontent.com/render/math?math=Q(%20w%2C%7Bw%7D_%7B0%7D)%20%3D%7B%5Csum%7D_%7Bi%3D1%7D%5E%7Bl%7D%20%7B(1-%7BM%7D_%7Bi%7D(%20w%2C%7Bw%7D_%7B0%7D))%7D_%7B%2B%7D%20%2B%5Cfrac%7B1%7D%7B2C%7D%7B%5ClVert%20w%5CrVert%7D%5E%7B2%7D%20%5Crightarrow%20%5Cmin%5Climits_%7Bw%2C%7Bw%7D_%7B0%7D%7D">

Функционал можно рассматривать как верхнюю оценку эмпирического риска (числа ошибочных классификаций объектов обучающей выборки), к которому добавлен *регуляризатор*

<img src="https://render.githubusercontent.com/render/math?math=%7B%7C%7Cw%7C%7C%7D%5E%7B2%7D">

умноженный на *параметр регуляризации*

<img src="https://render.githubusercontent.com/render/math?math=%5Cfrac%7B1%7D%7B2C%7D">

###### О подборе параметра регуляризации:

Константу C обычно выбирают по критерию скользящего контроля. Это трудоёмкий способ, так как задачу приходится решать заново при каждом значении C. Хорошо то, что решение, как правило, не очень чувствительно к выбору C, и слишком точная его оптимизация не требуется. Если есть основания полагать, что выборка почти линейно разделима, и лишь объекты-выбросы классифицируются неверно, то можно применить фильтрацию выбросов. Сначала задача решается при некотором C, и из выборки удаляется небольшая доля объектов, имеющих наибольшую величину ошибки. После этого задача решается заново по усечённой выборке. Возможно, придётся проделать несколько таких итераций, пока оставшиеся объекты не окажутся линейно разделимыми.

### Ядра и спрямляющие пространства

Существует ещё один подход к решению проблемы линейной неразделимости. Это переход от исходного пространства признаковых описаний объектов X к новому пространству H с помощью некоторого преобразования ψ. Если пространство H имеет достаточно высокую размерность, то можно надеяться, что в нём выборка окажется линейно разделимой. Пространство H называют *спрямляющим*.

Построение SVM в таком случае происходит так же, как и раньше. Единственное отличие состоит в том, что скалярное произведение ⟨x, x'⟩ в пространстве X всюду заменяется скалярным произведением ⟨ψ(x), ψ(x')⟩ в пространстве H. Отсюда вытекает естественное требование: пространство H должно быть наделено скалярным произведением, в частности, подойдёт любое евклидово, а в общем случае и гильбертово, пространство.

Постановка двойственной задачи и сам алгоритм классификации зависят только от скалярных произведений объектов, но не от самих признаковых описаний. Это означает, что скалярное произведение ⟨x, x'⟩ можно формально заменить ядром K(x, x′). Поскольку ядро в общем случае нелинейно, такая замена приводит к существенному расширению множества реализуемых алгоритмов. Более того, можно вообще не строить спрямляющее пространство H в явном виде, и вместо подбора отображения ψ заниматься непосредственно подбором ядра. Можно пойти ещё дальше, и вовсе отказаться от признаковых описаний объектов. Во многих практических задачах объекты изначально задаются информацией об их попарном взаимоотношении, например, отношении сходства. Если эта информация допускает представление в виде двуместной функции K(x, x′), удовлетворяющей аксиомам скалярного произведения, то задача может решаться методом SVM. Для такого подхода недавно был придуман термин *беспризнаковое распознавание* (featureless recognition).

###### SVM как двухслойная нейронная сеть:

Рассмотрим структуру алгоритма после замены скалярного произведения ⟨x, x'⟩ ядром K(x, x′). Перенумеруем
объекты так, чтобы первые h объектов оказались опорными. Тогда алгоритм примет вид

<img src="https://render.githubusercontent.com/render/math?math=a(x)%20%3D%5Coperatorname%7Bsign%7D(%20%7B%5Csum%7D_%7Bi%3D1%7D%5E%7Bh%7D%20%7B%5Clambda%7D_%7Bi%7D%20%7By%7D_%7Bi%7D%20K(%7Bx%7D_%7Bi%7D%2Cx)%20-%7Bw%7D_%7B0%7D)">

Если 

<img src="https://render.githubusercontent.com/render/math?math=X%3D%7B%5Cmathbb%7BR%7D%7D%5E%7Bn%7D">

то алгоритм можно рассматривать как суперпозицию, называемую *двухслойной нейронной сетью*. Первый слой образуют ядра, второй слой — собственно линейный классификатор. 

###### Преимущества SVM:

- Задача квадратичного программирования имеет единственное решение, для нахождения которого разработаны достаточно эффективные методы.
- Автоматически определяется сложность суперпозиции — число нейронов первого слоя, равное числу опорных векторов.
- Максимизация зазора между классами улучшает обобщающую способность.

###### Недостатки SVM:

- Неустойчивость к шуму в исходных данных. Объекты-выбросы являются опорными и существенно влияют на результат обучения.
- До сих пор не разработаны общие методы подбора ядер под конкретную задачу. На практике «вполне разумные» ядра, построенные с учётом специфики задачи, могут и не обладать свойством положительной определённости.
- Подбор параметра C требует многократного решения задачи.

###### Метод релевантных векторов (RVM):

Ещё одна нетривиальная идея регуляризации заключается в том, что может быть указан вид функциональной зависимости вектора параметров модели w от обучающей выборки и каких-то новых параметров λ. Тогда априорное распределение можно задавать не для w, а для λ. Поясним эту конструкцию на примере *метода релевантных векторов* (relevance vector machine, RVM).

За основу в RVM берётся формула

<img src="https://render.githubusercontent.com/render/math?math=w%3D%7B%5Csum%7D_%7Bi%3D1%7D%5E%7Bl%7D%20%7B%5Clambda%7D_%7Bi%7D%20%7By%7D_%7Bi%7D%20%7Bx%7D_%7Bi%7D">

и ставится задача определить, какие из коэффициентов можно положить равными нулю. Иными словами, делается попытка оптимизировать множество опорных объектов, сохранив свойство разреженности SVM. Для этого предполагается, что коэффициенты — независимые нормально распределённые случайные величины с неравными дисперсиями:

<img src="https://render.githubusercontent.com/render/math?math=%5Crho(%5Clambda)%20%3D%5Cfrac%7B1%7D%7B%7B(%202%5Cpi)%7D%5E%7Bl%2F2%7D%20%5Csqrt%7B%7B%5Calpha%7D_%7B1%7D%20%5Ccdots%20%7B%5Calpha%7D_%7Bl%7D%7D%7D%20%5Cexp(%20-%7B%5Csum%7D_%7Bi%3D1%7D%5E%7Bl%7D%20%5Cfrac%7B%7B%5Clambda%7D_%7Bi%7D%5E%7B2%7D%7D%7B2%7B%5Calpha%7D_%7Bi%7D%7D)">

То есть идея та же, что и в регуляризаторе, только теперь параметры априорного распределения связываются с объектами, а не с признаками. В результате вместо отбора признаков получаем отбор объектов, однако не такой, как в SVM, поэтому здесь опорные *объекты* называют *релевантными*. Эксперименты показали, что решение получается ещё более разреженным, чем в SVM, то есть релевантных объектов, как правило, существенно меньше, чем опорных. К сожалению, далеко не во всех задачах это действительно приводит к улучшению качества классификации.
